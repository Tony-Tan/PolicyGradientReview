import numpy as np
import torch.optim
import argparse
from core.rl_elements import *
from sklearn.utils import shuffle
import gym
import copy
import math


parser = argparse.ArgumentParser(description='PyTorch DDPG algorithm for continuous control environment')
parser.add_argument('--env_name', default='InvertedDoublePendulum-v2', type=str,
                    help='Mujoco Gym environment (default: InvertedDoublePendulum-v2)')
parser.add_argument('--critic_hidden_layer', default=[400, 300], nargs='+', type=int,
                    help='critic hidden perceptron size')
parser.add_argument('--actor_hidden_layer', default=[400, 300], nargs='+', type=int,
                    help='acot hidden perceptron size')
parser.add_argument('--critic_lr', default=1e-3, type=float, help='critic learning rate')
parser.add_argument('--actor_lr', default=1e-3, type=float, help='actor learning rate')
parser.add_argument('--batch_size', default=100, type=int, help='actor critic update batch_size')
parser.add_argument('--discounted_rate', default=0.99, type=float,
                    help='episodic return discounted rate $\\gamma$ in the paper')
parser.add_argument('--buffer_size', default=1000000, type=int, help='buffer size')
parser.add_argument('--steps_prepare_randomly', default=10000, type=int,
                    help='random generated steps before first update')
parser.add_argument('--steps_prepare', default=1000, type=int,  help='steps generated by actor before first update')
parser.add_argument('--polyak', default=.995, type=float, help='polyak coefficient')
parser.add_argument('--round_num', default=3000000, type=int,  help='how many round to play the game')
parser.add_argument('--recording_period', default=2000, type=int,  help='record checkpoints period')
parser.add_argument('--trajectory_size_each_round', default=100, type=int,
                    help='how many steps to play between each update')
parser.add_argument('--training_device', default='cpu', type=str,
                    help='training mlp on what device, could be mps, cpu, cuda')
parser.add_argument('--test_device', default='cpu', type=str,
                    help='test agent on what device, could be mps, cpu, cuda')
parser.add_argument('--agent_path', default='./data/models/checkpoint.pt', type=str,
                    help='agent models, actors, critics save path')
parser.add_argument('--experiment_log_path', default='./data/log', type=str,
                    help='experiment save path')
args = parser.parse_args()


class DDPGMLPCritic(Critic):
    def __init__(self, observation_space: gym.Space,
                 action_space: gym.Space,
                 hidden_layers_size: list, hidden_action):

        super(DDPGMLPCritic, self).__init__(observation_space, action_space)
        self.Linear_1 = torch.nn.Linear(self.obs_dim, hidden_layers_size[0])
        self.Linear_2 = torch.nn.Linear(self.action_dim+hidden_layers_size[0], hidden_layers_size[1])
        self.Linear_3 = torch.nn.Linear(hidden_layers_size[1], 1)
        self.hidden_action = hidden_action()

    def forward(self, obs: torch.Tensor, action: torch.Tensor):
        x_1 = self.Linear_1(obs)
        x_2 = self.hidden_action(x_1)
        x_3 = torch.cat((x_2, action), 1)
        x_4 = self.Linear_2(x_3)
        x_5 = self.hidden_action(x_4)
        output = self.Linear_3(x_5)
        return output


class DDPG_Agent(Agent):
    def __init__(self, observation_space, action_space, actor_mlp_hidden_layer,
                 critic_mlp_hidden_layer, path='./data/models'):
        super(DDPG_Agent, self).__init__('DDPG', path)
        self.actor = MLPGaussianActorManuSTD(observation_space, action_space, actor_mlp_hidden_layer, torch.nn.ReLU,
                                             output_action=torch.nn.Tanh)

        def init_weights(m):
            if isinstance(m, nn.Linear):
                m.weight.data.uniform_(-1e-3, 1e-3)
                m.bias.data.fill_(0.0001)
        self.actor.apply(init_weights)

        self.critic = DDPGMLPCritic(observation_space, action_space, critic_mlp_hidden_layer, torch.nn.ReLU)

        def init_weights(m):
            if isinstance(m, nn.Linear):
                m.weight.data.uniform_(-1e-4, 1e-4)
                m.bias.data.fill_(0.0001)
        self.critic.apply(init_weights)

        self.actor_tar = copy.deepcopy(self.actor)
        self.critic_tar = copy.deepcopy(self.critic)
        self.critic_loss = torch.nn.MSELoss()
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=args.critic_lr, weight_decay=0.01)
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=args.actor_lr)

    def save(self, epoch: int):
        self.checkpoint = {
            'epoch': epoch,
            'actor_state_dict': self.actor.state_dict(),
            'actor_tar_state_dict': self.actor_tar.state_dict(),
            'critic_state_dict': self.critic.state_dict(),
            'critic_tar_state_dict': self.critic_tar.state_dict(),
            'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
            'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
            'critic_loss': self.critic_loss
            }
        torch.save(self.checkpoint, self.path)

    def load(self):
        if os.path.exists(self.path):
            checkpoint = torch.load(self.path, map_location=torch.device(args.training_device))
            self.start_epoch = checkpoint['epoch']

            self.actor.load_state_dict(checkpoint['actor_state_dict'])
            self.actor_tar.load_state_dict(checkpoint['actor_tar_state_dict'])
            self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])

            self.critic.load_state_dict(checkpoint['critic_state_dict'])
            self.critic_tar.load_state_dict(checkpoint['critic_tar_state_dict'])
            self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])

            self.actor.eval()
            self.actor_tar.eval()
            self.critic.eval()
            self.critic_tar.eval()

    def update_actor_critic(self, epoch_num: int, data: DataBuffer, update_time: int, device: str,
                            log_writer: SummaryWriter):
        batch_size = args.batch_size
        gamma = args.discounted_rate
        data_sample = data.sample(batch_size * update_time)
        reward_tensor = torch.as_tensor(data_sample['reward'], dtype=torch.float32).to(device)
        termination_tensor = torch.as_tensor(data_sample['termination'], dtype=torch.float32).to(device)
        obs_tensor = torch.as_tensor(data_sample['obs'], dtype=torch.float32).to(device)
        next_obs_tensor = torch.as_tensor(data_sample['next_obs'], dtype=torch.float32).to(device)
        action_tensor = torch.as_tensor(data_sample['action'], dtype=torch.float32).to(device)
        self.actor.to(device)
        self.actor_tar.to(device)
        self.critic.to(device)
        self.critic_tar.to(device)

        average_residual = 0
        policy_loss = 0
        for i in range(update_time):
            start_ptr = i * batch_size
            end_ptr = (i + 1) * batch_size
            # update main networks
            with torch.no_grad():
                new_action_tensor, _ = self.actor_tar(next_obs_tensor[start_ptr:end_ptr])
                value_target = reward_tensor[start_ptr:end_ptr] + \
                    gamma * (1 - termination_tensor[start_ptr:end_ptr]) * \
                    self.critic_tar(next_obs_tensor[start_ptr:end_ptr], new_action_tensor)
            value_target.require_grad = False

            critic_output = self.critic(obs_tensor[start_ptr:end_ptr], action_tensor[start_ptr:end_ptr])
            critic_loss = self.critic_loss(value_target, critic_output)
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            self.critic_optimizer.step()
            average_residual += critic_loss.item()

            actor_output, _ = self.actor.act(obs_tensor[start_ptr:end_ptr], stochastically=False)
            for p in self.critic.parameters():
                p.requires_grad = False
            q_value = self.critic(obs_tensor[start_ptr:end_ptr], actor_output)
            average_q_value = - q_value.mean()
            self.actor_optimizer.zero_grad()
            average_q_value.backward()
            self.actor_optimizer.step()
            for p in self.critic.parameters():
                p.requires_grad = True
            policy_loss += average_q_value.item()
            # update target
            polyak_average(self.actor_tar, self.actor, args.polyak, self.actor_tar)
            polyak_average(self.critic_tar, self.critic, args.polyak, self.critic_tar)

        if epoch_num % 200 == 0:
            average_residual /= update_time
            print_time()
            print('\t\t regression state value for advantage; epoch: ' + str(epoch_num))
            print('\t\t value loss: ' + str(average_residual))
            print('\t\t policy loss: ' + str(policy_loss))
            print('-----------------------------------------------------------------')
            log_writer.add_scalar('loss/value_loss', average_residual, epoch_num)
            log_writer.add_scalar('loss/policy_loss', policy_loss, epoch_num)


class DDPG_exp(RLExperiment):
    def __init__(self, env, agent: Agent, buffer_size, discounted_rate,
                 normalize=True, log_path='./data/log', env_data_path='./data/models/'):
        buffer_template = {'obs': agent.actor.obs_dim, 'action': agent.actor.action_dim,
                           'next_obs': agent.actor.obs_dim, 'reward': 0, 'termination': 0}
        super(DDPG_exp, self).__init__(env, discounted_rate, agent, buffer_size, buffer_template, log_path)
        # self.buffer = DataBuffer(buffer_size, data_template)
        # self.trajectory_size = buffer_size
        self.reward_std = None
        self.state_std = None
        self.state_mean = None
        self.normalize = normalize
        if self.normalize:
            self.env_info(env_data_path)

    def generate_trajectories(self, trajectory_size: int, random_action=False):
        # current_state = self.env.reset()
        self.agent.actor.to('cpu')
        if self.buffer.ptr == 0 and not self.buffer.buffer_full_filled:
            current_state = self.env.reset()
            if self.normalize:
                current_state = np.float32((current_state - self.state_mean) / self.state_std)
        else:
            if self.buffer['termination'][self.buffer.ptr - 1]:
                current_state = self.env.reset()
                if self.normalize:
                    current_state = np.float32((current_state - self.state_mean) / self.state_std)
            else:
                current_state = self.buffer['next_obs'][self.buffer.ptr - 1]
        for _ in range(trajectory_size):
            current_state = torch.as_tensor(current_state, dtype=torch.float32)
            if not random_action:
                action, _ = self.agent.actor.act(current_state)
                action = action.detach().cpu().numpy()
            else:
                action = self.env.action_space.sample()
            new_state, reward, is_done, _ = self.env.step(action)
            if self.normalize:
                new_state = np.float32((new_state - self.state_mean) / self.state_std)
                self.buffer.push([current_state, action, new_state, reward / self.reward_std, is_done])
            else:
                self.buffer.push([current_state, action, new_state, reward, is_done])
            if is_done:
                current_state = self.env.reset()
                if self.normalize:
                    current_state = np.float32((current_state - self.state_mean) / self.state_std)
            else:
                current_state = new_state

    def env_info(self, data_path):
        if os.path.exists(os.path.join(data_path, 'state_mean.npy')):
            state_mean_path = os.path.join(data_path, 'state_mean.npy')
            self.state_mean = np.load(state_mean_path)
            state_std_path = os.path.join(data_path, 'state_std.npy')
            self.state_std = np.load(state_std_path)
            reward_std_path = os.path.join(data_path, 'reward_std.npy')
            self.reward_std = np.load(reward_std_path)
            print('data loaded .... ')
            print('state mean', self.state_mean)
            print('state std', self.state_std)
            print('reward std', self.reward_std)
        else:
            state_list = []
            discount_return_list = []
            for i in range(1000):
                state = self.env.reset()
                state_list.append(state)
                total_reward = 0
                step_i = 0
                while True:
                    random_action = self.env.action_space.sample()
                    new_state, reward, is_done, _ = self.env.step(random_action)
                    state_list.append(new_state)
                    total_reward += np.power(self.gamma, step_i) * reward
                    step_i += 1
                    if is_done:
                        break
                discount_return_list.append(total_reward)
            self.env.close()
            reward_np = np.array(discount_return_list, dtype=np.float32)
            state_np = np.array(state_list, dtype=np.float32)
            self.reward_std = np.std(reward_np) + 1e-5  # not be zero
            self.state_std = np.std(state_np, axis=0) + 1e-5  # not be zero
            self.state_mean = np.mean(state_np, axis=0)
            state_mean_path = os.path.join(data_path, 'state_mean.npy')
            np.save(state_mean_path, self.state_mean)
            state_std_path = os.path.join(data_path, 'state_std.npy')
            np.save(state_std_path, self.state_std)
            reward_std_path = os.path.join(data_path, 'reward_std.npy')
            np.save(reward_std_path, self.reward_std)
            print('state reward info saved!')

    def test(self, round_num: int, test_round_num: int, device):
        env = copy.deepcopy(self.env)
        total_reward = 0.0
        total_steps = 0
        self.agent.actor.eval()
        self.agent.actor.to(device)
        for i in range(test_round_num):
            obs = env.reset()
            while True:
                if self.normalize:
                    obs = np.float32((obs - self.state_mean) / self.state_std)
                obs_tensor = torch.as_tensor(obs, dtype=torch.float32).to(device)
                action, _ = self.agent.actor.act(obs_tensor, stochastically=False)
                action = action.detach().numpy()
                obs, reward, done, _ = env.step(action)
                total_reward += reward
                total_steps += 1
                if done:
                    break
        print("Episode done in %.2f steps, total reward %.2f" % (
            total_steps / test_round_num, total_reward / test_round_num))
        env.close()
        self.exp_log_writer.add_scalar('reward', total_reward / test_round_num, round_num)
        self.exp_log_writer.add_scalar('step', total_steps / test_round_num, round_num)
        return total_reward / test_round_num

    def play(self, round_num, trajectory_size_each_round, training_device, test_device, recording_period):
        start_round_num = self.agent.start_epoch + 1
        print_time()
        self.generate_trajectories(args.steps_prepare_randomly, random_action=True)
        self.generate_trajectories(args.steps_prepare)
        for round_i in range(start_round_num, round_num):
            self.generate_trajectories(trajectory_size_each_round)
            # with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:
            self.agent.update_actor_critic(round_i, self.buffer, trajectory_size_each_round,
                                           training_device, self.exp_log_writer)
            # print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=100))

            if round_i % recording_period == 0:
                self.test(round_i, 100, test_device)
                self.agent.save(round_i)
                # self.exp_log_writer.add_scalars('lr', {'value': self.agent.hyperparameter['critic_main_lr'],
                #                                        'policy': self.agent.hyperparameter['actor_main_lr']}, round_i)


if __name__ == '__main__':
    env_ = gym.make(args.env_name)
    agent_ = DDPG_Agent(env_.observation_space, env_.action_space, args.actor_hidden_layer,
                        args.critic_hidden_layer, args.agent_path)
    agent_.load()
    experiment = DDPG_exp(env_,  agent_, args.buffer_size, args.discounted_rate, True, args.experiment_log_path)
    experiment.play(args.round_num, args.trajectory_size_each_round, args.training_device,
                    args.test_device, args.recording_period)
    env_.close()
