import torch.optim
import gym
import copy

GAUSSIAN_NORMAL_CONSTANT = np.float32(1. / np.sqrt(2 * np.pi))
LOG_GAUSSIAN_NORMAL_CONSTANT = np.log(GAUSSIAN_NORMAL_CONSTANT)


parser = argparse.ArgumentParser(description='PyTorch ddpg algorithm for continuous control environment')
parser.add_argument('--env_name', default='InvertedDoublePendulum-v2', type=str,
                    help='Mujoco Gym environment (default: InvertedDoublePendulum-v2)')
parser.add_argument('--critic_hidden_layer', default=[400, 300], nargs='+', type=int,
                    help='critic hidden perceptron size')
parser.add_argument('--actor_hidden_layer', default=[400, 300], nargs='+', type=int,
                    help='acot hidden perceptron size')
parser.add_argument('--critic_lr', default=1e-3, type=float, help='critic learning rate')
parser.add_argument('--actor_lr', default=1e-4, type=float, help='actor learning rate')
parser.add_argument('--initialize_nn', default=True, type=bool,
                    help='initialize models to the range suggested by the paper')
parser.add_argument('--batch_size', default=100, type=int, help='actor critic update batch_size')
parser.add_argument('--discounted_rate', default=0.99, type=float,
                    help='episodic return discounted rate $\\gamma$ in the paper')
parser.add_argument('--buffer_size', default=1000000, type=int, help='buffer size')
parser.add_argument('--steps_prepare_randomly', default=10000, type=int,
                    help='random generated steps before first update')
parser.add_argument('--steps_prepare', default=1000, type=int, help='steps generated by actor before first update')
parser.add_argument('--polyak', default=.995, type=float, help='polyak coefficient')
parser.add_argument('--round_num', default=3000000, type=int, help='how many round to play the game')
parser.add_argument('--recording_period', default=2000, type=int, help='record checkpoints period')
parser.add_argument('--trajectory_size_each_round', default=100, type=int,
                    help='how many steps to play between each update')
parser.add_argument('--update_times_per_learning', default=100, type=int,
                    help='how many updates per learning periodic')
parser.add_argument('--training_device', default='cpu', type=str,
                    help='training mlp on what device, could be mps, cpu, cuda')
parser.add_argument('--test_device', default='cpu', type=str,
                    help='tests agent on what device, could be mps, cpu, cuda')
parser.add_argument('--agent_path', default='./data/models/checkpoint.pt', type=str,
                    help='agent models, actors, critics save path')
parser.add_argument('--experiment_log_path', default='./data/log', type=str,
                    help='experiment save path')
args = parser.parse_args()



# class Actor(MLPGaussian):
#     def __init__(self, obs_dim: int, action_dim: int,
#                  hidden_layers_size: list, hidden_action,
#                  output_action=torch.nn.Identity):
#         super(Actor, self).__init__(obs_dim, action_dim,
#                                     hidden_layers_size, hidden_action,
#                                     output_action)


# def update_std(self):
#     self.std = self.std * self.std_decay
#
# def act(self, x: torch.Tensor, stochastically=True, with_log_pro=True):
#     mu, std = self.forward(x)
#     mu = mu + self.action_mean
#     log_pro = torch.zeros_like(mu)
#     if stochastically:
#         action_noise = torch.randn_like(mu)
#         actions = action_noise * std + mu
#         action = torch.clamp(actions, min=self.action_mean - self.action_radius,
#                              max=self.action_mean + self.action_radius)
#         if with_log_pro:
#             log_pro = LOG_GAUSSIAN_NORMAL_CONSTANT - np.log(std) - \
#                       0.5 * (action - mu) * (action - mu).sum(axis=-1, keepdims=True)
#         return action, log_pro
#
#     else:
#         return mu, log_pro


class DDPG(Agent):
    def __init__(self, observation_space: gym.Space, action_space: gym.Space):
        super(DDPG, self).__init__(args.buffer_size)
        self.obs_dim = observation_space.shape[-1]
        self.action_dim = action_space.shape[-1]
        self.action_low = torch.tensor(np.array([i for i in action_space.low]), dtype=torch.float32)
        self.action_high = torch.tensor(np.array([i for i in action_space.high]), dtype=torch.float32)
        self.action_mean = torch.as_tensor((self.action_low + self.action_high) / 2.0, dtype=torch.float32)
        self.action_radius = torch.as_tensor((self.action_high - self.action_low) / 2.0, dtype=torch.float32)

        self.actor = MLPGaussian(self.obs_dim, self.action_dim, args.actor_hidden_layer,
                                 torch.nn.Tanh, output_action=torch.nn.Tanh)
        self.actor_std = 0.1 * torch.ones_like(self.action_mean)

        if args.initialize_nn:
            def init_weights(m):
                if isinstance(m, torch.nn.Linear):
                    m.weight.data.uniform_(-3e-3, 3e-3)
                    m.bias.data.fill_(0.0001)

            self.actor.apply(init_weights)

        # self.critic = DDPGMLPCritic(observation_space, action_space, critic_mlp_hidden_layer, torch.nn.ReLU)
        self.critic = MLPCritic(self.obs_dim, self.action_dim,
                                args.critic_hidden_layer, torch.nn.Tanh)
        if args.initialize_nn:
            def init_weights(m):
                if isinstance(m, torch.nn.Linear):
                    m.weight.data.uniform_(-3e-4, 3e-4)
                    m.bias.data.fill_(0.0001)

            self.critic.apply(init_weights)

        self.actor_tar = copy.deepcopy(self.actor)
        self.critic_tar = copy.deepcopy(self.critic)
        self.critic_loss = torch.nn.MSELoss()
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=args.critic_lr, weight_decay=0.01)
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=args.actor_lr)

    def react(self, obs: np.ndarray, testing: bool = False):
        mu = self.actor(obs)
        mu = mu + self.action_mean
        log_pro = torch.zeros_like(mu)
        if not testing:
            action_noise = torch.randn_like(mu)
            actions = action_noise * self.actor_std + mu
            action = torch.clamp(actions, min=self.action_mean - self.action_radius,
                                 max=self.action_mean + self.action_radius)
            log_pro = LOG_GAUSSIAN_NORMAL_CONSTANT - np.log(self.actor_std) - 0.5 * \
                      (action - mu) * (action - mu).sum(axis=-1, keepdims=True)
            return action, log_pro

        else:
            return mu, log_pro

    def learn(self):
        batch_size = args.batch_size
        update_times_per_learning = args.update_times_per_learning
        gamma = args.discounted_rate
        data_sample = self.memory.sample(batch_size * update_times_per_learning)
        device = args.training_device
        reward_tensor = torch.as_tensor(data_sample['reward'], dtype=torch.float32).to(device)
        termination_tensor = torch.as_tensor(data_sample['termination'], dtype=torch.float32).to(device)
        obs_tensor = torch.as_tensor(data_sample['obs'], dtype=torch.float32).to(device)
        next_obs_tensor = torch.as_tensor(data_sample['next_obs'], dtype=torch.float32).to(device)
        action_tensor = torch.as_tensor(data_sample['action'], dtype=torch.float32).to(device)
        self.actor.to(device)
        self.actor_tar.to(device)
        self.critic.to(device)
        self.critic_tar.to(device)

        average_residual = 0
        policy_loss = 0
        for i in range(update_times_per_learning):
            start_ptr = i * batch_size
            end_ptr = (i + 1) * batch_size
            # update main models
            with torch.no_grad():
                new_action_tensor, _ = self.actor_tar.act(next_obs_tensor[start_ptr:end_ptr], stochastically=False)
                value_target = reward_tensor[start_ptr:end_ptr] + \
                               gamma * (1 - termination_tensor[start_ptr:end_ptr]) * \
                               self.critic_tar(next_obs_tensor[start_ptr:end_ptr], new_action_tensor)
            value_target.require_grad = False

            critic_output = self.critic(obs_tensor[start_ptr:end_ptr], action_tensor[start_ptr:end_ptr])
            critic_loss = self.critic_loss(value_target, critic_output)
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            self.critic_optimizer.step()
            average_residual += critic_loss.item()

            actor_output, _ = self.react(obs_tensor[start_ptr:end_ptr], False)
            for p in self.critic.parameters():
                p.requires_grad = False
            q_value = self.critic(obs_tensor[start_ptr:end_ptr], actor_output)
            average_q_value = - q_value.mean()
            self.actor_optimizer.zero_grad()
            average_q_value.backward()
            self.actor_optimizer.step()
            for p in self.critic.parameters():
                p.requires_grad = True
            policy_loss += average_q_value.item()
            # update target
            polyak_average(self.actor_tar, self.actor, args.polyak, self.actor_tar)
            polyak_average(self.critic_tar, self.critic, args.polyak, self.critic_tar)

        if epoch_num % 200 == 0:
            average_residual /= update_time
            policy_loss /= update_time
            print_time()
            print('\t\t regression state value for advantage; epoch: ' + str(epoch_num))
            print('\t\t value loss: ' + str(average_residual))
            print('\t\t policy loss: ' + str(policy_loss))
            print('-----------------------------------------------------------------')
            log_writer.add_scalar('loss/value_loss', average_residual, epoch_num)
            log_writer.add_scalar('loss/policy_loss', policy_loss, epoch_num)
