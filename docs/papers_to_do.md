## Papers To Code

| State | Title                                                                                                                                                                                                                                                                                                                    | Path |
|:-----:|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----:|
|       | Adeniji, Ademi, Amber Xie, Carmelo Sferrazza, Younggyo Seo, Stephen James, and Pieter Abbeel. 2023. “Language Reward Modulation for Pretraining Reinforcement Learning.” arXiv. http://arxiv.org/abs/2308.12270.                                                                                                         |      |
|       | Afshar, Aida, and Wenchao Li. 2024. “DeLF: Designing Learning Environments with Foundation Models.” arXiv. http://arxiv.org/abs/2401.08936.                                                                                                                                                                              |      |
|       | Alboni, Elisa, Gianluigi Grandesso, Gastone Pietro Rosati Papini, Justin Carpentier, and Andrea Del Prete. 2023. “CACTO-SL: Using Sobolev Learning to Improve Continuous Actor-Critic with Trajectory Optimization.” arXiv. http://arxiv.org/abs/2312.10666.                                                             |      |
|       | Anthony, Thomas, Zheng Tian, and David Barber. 2017. “Thinking Fast and Slow with Deep Learning and Tree Search.” arXiv:1705.08439 [Cs], December. http://arxiv.org/abs/1705.08439.                                                                                                                                      |      |
|       | Ba, Jimmy, Volodymyr Mnih, and Koray Kavukcuoglu. 2015. “Multiple Object Recognition with Visual Attention.” arXiv. http://arxiv.org/abs/1412.7755.                                                                                                                                                                      |      |
|       | Badalian, Kevin, Lucas Koch, Tobias Brinkmann, Mario Picerno, Marius Wegener, Sung-Yong Lee, and Jakob Andert. 2023. “LExCI: A Framework for Reinforcement Learning with Embedded Systems.” arXiv. http://arxiv.org/abs/2312.02739.                                                                                      |      |
|       | Badia, Adrià Puigdomènech, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, and Charles Blundell. 2020. “Agent57: Outperforming the Atari Human Benchmark.” arXiv. http://arxiv.org/abs/2003.13350.                                                                                        |      |
|       | Baker, Bowen, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. 2017. “Designing Neural Network Architectures Using Reinforcement Learning.” arXiv. http://arxiv.org/abs/1611.02167.                                                                                                                                        |      |
|       | Banerjee, Rohan, Prishita Ray, and Mark Campbell. 2023. “Improving Environment Robustness of Deep Reinforcement Learning Approaches for Autonomous Racing Using Bayesian Optimization-Based Curriculum Learning.” arXiv. http://arxiv.org/abs/2312.10557.                                                                |      |
|       | Bao, Wenhang, and Xiao-yang Liu. 2019. “Multi-Agent Deep Reinforcement Learning for Liquidation Strategy Analysis.” arXiv. http://arxiv.org/abs/1906.11046.                                                                                                                                                              |      |
|       | Barth-Maron, Gabriel, Matthew W. Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva TB, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. 2018. “Distributed Distributional Deterministic Policy Gradients.” arXiv:1804.08617 [Cs, Stat], April. http://arxiv.org/abs/1804.08617.                               |      |
|       | Beck, Andrea, and Marius Kurz. 2023. “Toward Discretization-Consistent Closure Schemes for Large Eddy Simulation Using Reinforcement Learning.” Physics of Fluids 35 (12): 125122. https://doi.org/10.1063/5.0176223.                                                                                                    |      |
|       | Beck, Jacob, Risto Vuorio, Zheng Xiong, and Shimon Whiteson. 2023. “Recurrent Hypernetworks Are Surprisingly Strong in Meta-RL.” arXiv. http://arxiv.org/abs/2309.14970.                                                                                                                                                 |      |
|       | Behzadan, Vahid, and Arslan Munir. 2017. “Whatever Does Not Kill Deep Reinforcement Learning, Makes It Stronger.” arXiv. http://arxiv.org/abs/1712.09344.                                                                                                                                                                |      |
|       | Ben-Kish, Assaf, Moran Yanuka, Morris Alper, Raja Giryes, and Hadar Averbuch-Elor. 2024. “Mitigating Open-Vocabulary Caption Hallucinations.” arXiv. http://arxiv.org/abs/2312.03631.                                                                                                                                    |      |
|       | Bester, Craig J., Steven D. James, and George D. Konidaris. 2019. “Multi-Pass Q-Networks for Deep Reinforcement Learning with Parameterised Action Spaces.” arXiv. http://arxiv.org/abs/1905.04388.                                                                                                                      |      |
|       | Bukharin, Alexander, Yixiao Li, Pengcheng He, Weizhu Chen, and Tuo Zhao. 2023. “Deep Reinforcement Learning from Hierarchical Weak Preference Feedback.” arXiv. http://arxiv.org/abs/2309.02632.                                                                                                                         |      |
|       | Bussola, Riccardo, Michele Focchi, Andrea Del Prete, Daniele Fontanelli, and Luigi Palopoli. 2023. “Efficient Reinforcement Learning for Jumping Monopods.” arXiv. http://arxiv.org/abs/2309.07038.                                                                                                                      |      |
|       | Campero, Andres, Roberta Raileanu, Heinrich Küttler, Joshua B. Tenenbaum, Tim Rocktäschel, and Edward Grefenstette. 2021. “Learning with AMIGo: Adversarially Motivated Intrinsic Goals.” arXiv. http://arxiv.org/abs/2006.12122.                                                                                        |      |
|       | Chaffin, Antoine, Ewa Kijak, and Vincent Claveau. 2024. “Distinctive Image Captioning: Leveraging Ground Truth Captions in CLIP Guided Reinforcement Learning.” arXiv. http://arxiv.org/abs/2402.13936.                                                                                                                  |      |
|       | Chen, Changan, Yuejiang Liu, Sven Kreiss, and Alexandre Alahi. 2019. “Crowd-Robot Interaction: Crowd-Aware Robot Navigation with Attention-Based Deep Reinforcement Learning.” arXiv. http://arxiv.org/abs/1809.08835.                                                                                                   |      |
|       | Chen, Jianyu, Shengbo Eben Li, and Masayoshi Tomizuka. 2020. “Interpretable End-to-End Urban Autonomous Driving with Latent Deep Reinforcement Learning.” arXiv. http://arxiv.org/abs/2001.08726.                                                                                                                        |      |
|       | Chen, Pengguang, Shu Liu, Hengshuang Zhao, Xingquan Wang, and Jiaya Jia. 2024. “GridMask Data Augmentation.” arXiv. http://arxiv.org/abs/2001.04086.                                                                                                                                                                     |      |
|       | Chen, Yinda, Wei Huang, Shenglong Zhou, Qi Chen, and Zhiwei Xiong. 2023. “Self-Supervised Neuron Segmentation with Multi-Agent Reinforcement Learning.” In Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, 609–17. https://doi.org/10.24963/ijcai.2023/68.                   |      |
|       | Chen, Zhipeng, Kun Zhou, Wayne Xin Zhao, Junchen Wan, Fuzheng Zhang, Di Zhang, and Ji-Rong Wen. 2024. “Improving Large Language Models via Fine-Grained Reinforcement Learning with Minimum Editing Constraint.” arXiv. http://arxiv.org/abs/2401.06081.                                                                 |      |
|       | Christiano, Paul, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2023. “Deep Reinforcement Learning from Human Preferences.” arXiv. http://arxiv.org/abs/1706.03741.                                                                                                                              |      |
|       | Clemente, Alfredo V., Humberto N. Castejón, and Arjun Chandra. 2017. “Efficient Parallel Methods for Deep Reinforcement Learning.” arXiv:1705.04862 [Cs], May. http://arxiv.org/abs/1705.04862.                                                                                                                          |      |
|       | Cobbe, Karl, Christopher Hesse, Jacob Hilton, and John Schulman. 2020. “Leveraging Procedural Generation to Benchmark Reinforcement Learning.” arXiv. http://arxiv.org/abs/1912.01588.                                                                                                                                   |      |
|       | Co-Reyes, John D., Yingjie Miao, Daiyi Peng, Esteban Real, Sergey Levine, Quoc V. Le, Honglak Lee, and Aleksandra Faust. 2022. “Evolving Reinforcement Learning Algorithms.” arXiv. http://arxiv.org/abs/2101.03958.                                                                                                     |      |
|       | Cui, Jing, Yufei Han, Yuzhe Ma, Jianbin Jiao, and Junge Zhang. 2023. “BadRL: Sparse Targeted Backdoor Attack Against Reinforcement Learning.” arXiv. http://arxiv.org/abs/2312.12585.                                                                                                                                    |      |
|       | Da, Longchao, Minquan Gao, Hao Mei, and Hua Wei. 2024. “Prompt to Transfer: Sim-to-Real Transfer for Traffic Signal Control with Prompt Learning.” arXiv. http://arxiv.org/abs/2308.14284.                                                                                                                               |      |
|       | Dalal, Gal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin Paduraru, and Yuval Tassa. 2018. “Safe Exploration in Continuous Action Spaces.” arXiv:1801.08757 [Cs], January. http://arxiv.org/abs/1801.08757.                                                                                                |      |
|       | Das, Abhishek, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. 2017. “Embodied Question Answering.” arXiv. http://arxiv.org/abs/1711.11543.                                                                                                                                                    |      |
|       | Das, Abhishek, Satwik Kottur, José M. F. Moura, Stefan Lee, and Dhruv Batra. 2017. “Learning Cooperative Visual Dialog Agents with Deep Reinforcement Learning.” arXiv. http://arxiv.org/abs/1703.06585.                                                                                                                 |      |
|       | Delfosse, Quentin, Sebastian Sztwiertnia, Mark Rothermel, Wolfgang Stammer, and Kristian Kersting. 2024. “Interpretable Concept Bottlenecks to Align Reinforcement Learning Agents.” arXiv. http://arxiv.org/abs/2401.05821.                                                                                             |      |
|       | Dennis, Michael, Natasha Jaques, Eugene Vinitsky, Alexandre Bayen, Stuart Russell, Andrew Critch, and Sergey Levine. 2021. “Emergent Complexity and Zero-Shot Transfer via Unsupervised Environment Design.” arXiv. http://arxiv.org/abs/2012.02096.                                                                     |      |
|       | Dietterich, Thomas G. 1999. “Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition.” arXiv. http://arxiv.org/abs/cs/9905014.                                                                                                                                                                    |      |
|       | Ding, Li, Jenny Zhang, Jeff Clune, Lee Spector, and Joel Lehman. 2023. “Quality Diversity through Human Feedback.” arXiv. http://arxiv.org/abs/2310.12103.                                                                                                                                                               |      |
|       | Ding, Shutong, Jingya Wang, Yali Du, and Ye Shi. 2023. “Reduced Policy Optimization for Continuous Control with Hard Constraints.” arXiv. http://arxiv.org/abs/2310.09574.                                                                                                                                               |      |
|       | Dou, Yingtong, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S. Yu. 2020. “Enhancing Graph Neural Network-Based Fraud Detectors against Camouflaged Fraudsters.” In Proceedings of the 29th ACM International Conference on Information & Knowledge Management, 315–24. https://doi.org/10.1145/3340531.3411903. |      |
|       | Duan, Jingliang, Wenxuan Wang, Liming Xiao, Jiaxin Gao, and Shengbo Eben Li. 2023. “DSAC-T: Distributional Soft Actor-Critic with Three Refinements.” arXiv. http://arxiv.org/abs/2310.05858.                                                                                                                            |      |
|       | E, Weinan, Jiequn Han, and Arnulf Jentzen. 2017. “Deep Learning-Based Numerical Methods for High-Dimensional Parabolic Partial Differential Equations and Backward Stochastic Differential Equations.” Communications in Mathematics and Statistics 5 (4): 349–80. https://doi.org/10.1007/s40304-017-0117-6.            |      |
|       | Everett, Michael, Yu Fan Chen, and Jonathan P. How. 2018. “Motion Planning Among Dynamic, Decision-Making Agents with Deep Reinforcement Learning.” arXiv. http://arxiv.org/abs/1805.01956.                                                                                                                              |      |
|       | Felten, Florian, Daniel Gareev, El-Ghazali Talbi, and Grégoire Danoy. 2023. “Hyperparameter Optimization for Multi-Objective Reinforcement Learning.” arXiv. http://arxiv.org/abs/2310.16487.                                                                                                                            |      |
|       | Foerster, Jakob, Nantas Nardelli, Gregory Farquhar, Triantafyllos Afouras, Philip H. S. Torr, Pushmeet Kohli, and Shimon Whiteson. 2018. “Stabilising Experience Replay for Deep Multi-Agent Reinforcement Learning.” arXiv. http://arxiv.org/abs/1702.08887.                                                            |      |
|       | Fridman, Lex, Jack Terwilliger, and Benedikt Jenik. 2019. “DeepTraffic: Crowdsourced Hyperparameter Tuning of Deep Reinforcement Learning Systems for Multi-Agent Dense Traffic Navigation.” arXiv. http://arxiv.org/abs/1801.02805.                                                                                     |      |
|       | Fu, Justin, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. 2020. “D4rl: Datasets for Deep Data-Driven Reinforcement Learning.” arXiv Preprint arXiv:2004.07219.                                                                                                                                            |      |
|       | Fu, Justin, Katie Luo, and Sergey Levine. 2018. “Learning Robust Rewards with Adversarial Inverse Reinforcement Learning.” arXiv. http://arxiv.org/abs/1710.11248.                                                                                                                                                       |      |
|       | Fujimoto, Scott, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. 2019. “Benchmarking Batch Deep Reinforcement Learning Algorithms.” arXiv. https://doi.org/10.48550/arXiv.1910.01708.                                                                                                                            |      |
|       | Gajcin, Jasmina, James McCarthy, Rahul Nair, Radu Marinescu, Elizabeth Daly, and Ivana Dusparic. 2023. “Iterative Reward Shaping Using Human Feedback for Correcting Reward Misspecification.” arXiv. http://arxiv.org/abs/2308.15969.                                                                                   |      |
|       | Gal, Yarin, Jiri Hron, and Alex Kendall. 2017. “Concrete Dropout.” arXiv. http://arxiv.org/abs/1705.07832.                                                                                                                                                                                                               |      |
|       | Gorinski, Philip John, Matthieu Zimmer, Gerasimos Lampouras, Derrick Goh Xin Deik, and Ignacio Iacobacci. 2023. “Automatic Unit Test Data Generation and Actor-Critic Reinforcement Learning for Code Synthesis.” arXiv. http://arxiv.org/abs/2310.13669.                                                                |      |
|       | Gou, Qi, Zehua Xia, Bowen Yu, Haiyang Yu, Fei Huang, Yongbin Li, and Nguyen Cam-Tu. 2023. “Diversify Question Generation with Retrieval-Augmented Style Transfer.” arXiv. http://arxiv.org/abs/2310.14503.                                                                                                               |      |
|       | Grathwohl, Will, Dami Choi, Yuhuai Wu, Geoffrey Roeder, and David Duvenaud. 2018. “Backpropagation through the Void: Optimizing Control Variates for Black-Box Gradient Estimation.” arXiv. http://arxiv.org/abs/1711.00123.                                                                                             |      |
|       | Grigsby, Jake, Linxi Fan, and Yuke Zhu. 2024. “AMAGO: Scalable In-Context Reinforcement Learning for Adaptive Agents.” arXiv. http://arxiv.org/abs/2310.09971.                                                                                                                                                           |      |
|       | Gupta, Nikunj, Anh Mai, Azza Abouzied, and Dennis Shasha. 2023. “On the Calibration of Compartmental Epidemiological Models.” arXiv. http://arxiv.org/abs/2312.05456.                                                                                                                                                    |      |
|       | Hafner, Danijar, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. 2023. “Mastering Diverse Domains through World Models.” arXiv. http://arxiv.org/abs/2301.04104.                                                                                                                                                      |      |
|       | Han, Jiuzhou, Wray Buntine, and Ehsan Shareghi. 2024. “Reward Engineering for Generating Semi-Structured Explanation.” arXiv. http://arxiv.org/abs/2309.08347.                                                                                                                                                           |      |
|       | Hausknecht, Matthew, and Peter Stone. 2016. “Deep Reinforcement Learning in Parameterized Action Space.” arXiv. http://arxiv.org/abs/1511.04143.                                                                                                                                                                         |      |
|       | Heess, Nicolas, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Greg Wayne, Yuval Tassa, et al. 2017. “Emergence of Locomotion Behaviours in Rich Environments.” arXiv:1707.02286 [Cs], July. http://arxiv.org/abs/1707.02286.                                                                                     |      |
|       | Heinrich, Johannes, and David Silver. 2016. “Deep Reinforcement Learning from Self-Play in Imperfect-Information Games.” arXiv. http://arxiv.org/abs/1603.01121.                                                                                                                                                         |      |
|       | Hejna, Joey, Rafael Rafailov, Harshit Sikchi, Chelsea Finn, Scott Niekum, W. Bradley Knox, and Dorsa Sadigh. 2023. “Contrastive Preference Learning: Learning from Human Feedback without RL.” arXiv. http://arxiv.org/abs/2310.13639.                                                                                   |      |
|       | Henderson, Peter, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. 2019. “Deep Reinforcement Learning That Matters.” arXiv:1709.06560 [Cs, Stat], January. http://arxiv.org/abs/1709.06560.                                                                                                  |      |
|       | Hester, Todd, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, et al. 2017. “Deep Q-Learning from Demonstrations.” arXiv:1704.03732 [Cs], November. http://arxiv.org/abs/1704.03732.                                                                                                   |      |
|       | Hiraoka, Takuya. 2023. “Efficient Sparse-Reward Goal-Conditioned Reinforcement Learning with a High Replay Ratio and Regularization.” arXiv. http://arxiv.org/abs/2312.05787.                                                                                                                                            |      |
|       | Hodson, Rowan, Bruce Bassett, Charel van Hoof, Benjamin Rosman, Mark Solms, Jonathan P. Shock, and Ryan Smith. 2023. “Planning to Learn: A Novel Algorithm for Active Learning during Model-Based Planning.” arXiv. http://arxiv.org/abs/2308.08029.                                                                     |      |
|       | Hoffman, Matthew W., Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Nikola Momchev, Danila Sinopalnikov, Piotr Stańczyk, et al. 2022. “Acme: A Research Framework for Distributed Reinforcement Learning.” arXiv. http://arxiv.org/abs/2006.00979.                                                                |      |
|       | Hong, Zhang-Wei, Aviral Kumar, Sathwik Karnik, Abhishek Bhandwaldar, Akash Srivastava, Joni Pajarinen, Romain Laroche, Abhishek Gupta, and Pulkit Agrawal. 2023. “Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets.” arXiv. http://arxiv.org/abs/2310.04413.                             |      |
|       | Hove, Alouette van, Kristoffer Aalstad, and Norbert Pirk. 2023. “Using Reinforcement Learning to Improve Drone-Based Inference of Greenhouse Gas Fluxes.” Nordic Machine Intelligence 2 (4): 6. https://doi.org/10.5617/nmi.9897.                                                                                        |      |
|       | Hu, Hengyuan, and Jakob N. Foerster. 2021. “Simplified Action Decoder for Deep Multi-Agent Reinforcement Learning.” arXiv. http://arxiv.org/abs/1912.02288.                                                                                                                                                              |      |
|       | Hu, Jian, Li Tao, June Yang, and Chandler Zhou. 2023. “Aligning Language Models with Offline Learning from Human Feedback.” arXiv. http://arxiv.org/abs/2308.12050.                                                                                                                                                      |      |
|       | Huang, Shiyu, Wentse Chen, Yiwen Sun, Fuqing Bie, and Wei-Wei Tu. 2023. “OpenRL: A Unified Reinforcement Learning Framework.” arXiv. http://arxiv.org/abs/2312.16189.                                                                                                                                                    |      |
|       | Huang, Wenhui, Cong Zhang, Jingda Wu, Xiangkun He, Jie Zhang, and Chen Lv. 2023. “Sampling Efficient Deep Reinforcement Learning Through Preference-Guided Stochastic Exploration.” IEEE Transactions on Neural Networks and Learning Systems PP (October): 1–12. https://doi.org/10.1109/TNNLS.2023.3317628.            |      |
|       | Huang, Xingshuai, Di Wu, and Benoit Boulet. 2023. “Traffic Signal Control Using Lightweight Transformers: An Offline-to-Online RL Approach.” arXiv. http://arxiv.org/abs/2312.07795.                                                                                                                                     |      |
|       | Huang, Zhewei, Wen Heng, and Shuchang Zhou. 2019. “Learning to Paint With Model-Based Deep Reinforcement Learning.” arXiv. http://arxiv.org/abs/1903.04411.                                                                                                                                                              |      |
|       | Iklassov, Zangir, Ikboljon Sobirov, Ruben Solozabal, and Martin Takac. 2023. “Reinforcement Learning for Solving Stochastic Vehicle Routing Problem.” arXiv. http://arxiv.org/abs/2311.07708.                                                                                                                            |      |
|       | Jackson, Matthew Thomas, Minqi Jiang, Jack Parker-Holder, Risto Vuorio, Chris Lu, Gregory Farquhar, Shimon Whiteson, and Jakob Nicolaus Foerster. 2023. “Discovering General Reinforcement Learning Algorithms with Adversarial Environment Design.” arXiv. http://arxiv.org/abs/2310.02782.                             |      |
|       | Jang, Doseok, Larry Yan, Lucas Spangher, and Costas Spanos. 2023. “Active Reinforcement Learning for Robust Building Control.” arXiv. http://arxiv.org/abs/2312.10289.                                                                                                                                                   |      |
|       | Jeen, Scott, Tom Bewley, and Jonathan M. Cullen. 2023. “Conservative World Models.” arXiv. http://arxiv.org/abs/2309.15178.                                                                                                                                                                                              |      |
|       | Jiang, Jiechuan, Chen Dun, Tiejun Huang, and Zongqing Lu. 2020. “Graph Convolutional Reinforcement Learning.” arXiv. http://arxiv.org/abs/1810.09202.                                                                                                                                                                    |      |
|       | Karimpanal, Thommen George, Laknath Buddhika Semage, Santu Rana, Hung Le, Truyen Tran, Sunil Gupta, and Svetha Venkatesh. 2023. “LaGR-SEQ: Language-Guided Reinforcement Learning with Sample-Efficient Querying.” arXiv. http://arxiv.org/abs/2308.13542.                                                               |      |
|       | Keller, Yannik, Jannis Blüml, Gopika Sudhakaran, and Kristian Kersting. 2023. “From Images to Connections: Can DQN with GNNs Learn the Strategic Game of Hex?” arXiv. http://arxiv.org/abs/2311.13414.                                                                                                                   |      |
|       | Kendall, Alex, Jeffrey Hawke, David Janz, Przemyslaw Mazur, Daniele Reda, John-Mark Allen, Vinh-Dieu Lam, Alex Bewley, and Amar Shah. 2018. “Learning to Drive in a Day.” arXiv. http://arxiv.org/abs/1807.00412.                                                                                                        |      |
|       | Khadka, Shauharda, and Kagan Tumer. 2018. “Evolution-Guided Policy Gradient in Reinforcement Learning.” arXiv. http://arxiv.org/abs/1805.07917.                                                                                                                                                                          |      |
|       | Khalifa, Ahmed, Philip Bontrager, Sam Earle, and Julian Togelius. 2020. “PCGRL: Procedural Content Generation via Reinforcement Learning.” arXiv. http://arxiv.org/abs/2001.09212.                                                                                                                                       |      |
|       | Kim, Heasung, and Sravan Kumar Ankireddy. 2023. “Learning RL-Policies for Joint Beamforming Without Exploration: A Batch Constrained Off-Policy Approach.” arXiv. http://arxiv.org/abs/2310.08660.                                                                                                                       |      |
|       | Kim, Seongun, Kyowoon Lee, and Jaesik Choi. 2023. “Variational Curriculum Reinforcement Learning for Unsupervised Discovery of Skills.” arXiv. http://arxiv.org/abs/2310.19424.                                                                                                                                          |      |
|       | Kim, Woojun, Jeonghye Kim, and Youngchul Sung. 2023. “LESSON: Learning to Integrate Exploration Strategies for Reinforcement Learning via an Option Framework.” arXiv. http://arxiv.org/abs/2310.03342.                                                                                                                  |      |
|       | Klissarov, Martin, Pierluca D’Oro, Shagun Sodhani, Roberta Raileanu, Pierre-Luc Bacon, Pascal Vincent, Amy Zhang, and Mikael Henaff. 2023. “Motif: Intrinsic Motivation from Artificial Intelligence Feedback.” arXiv. http://arxiv.org/abs/2310.00166.                                                                  |      |
|       | Klu, Emmanuel, Sameer Sethi, D. J. Passey, and Donald Martin Jr. 2023. “SDGym: Low-Code Reinforcement Learning Environments Using System Dynamics Models.” arXiv. http://arxiv.org/abs/2310.12494.                                                                                                                       |      |
|       | Kostrikov, Ilya, Denis Yarats, and Rob Fergus. 2021. “Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels.” arXiv. http://arxiv.org/abs/2004.13649.                                                                                                                                 |      |
|       | Kuba, Jakub Grudzien, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, and Yaodong Yang. 2022. “Trust Region Policy Optimisation in Multi-Agent Reinforcement Learning.” arXiv. http://arxiv.org/abs/2109.11251.                                                                                               |      |
|       | Kulkarni, Tejas D., Karthik R. Narasimhan, Ardavan Saeedi, and Joshua B. Tenenbaum. 2016. “Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation.” arXiv. http://arxiv.org/abs/1604.06057.                                                                                 |      |
|       | Kulkarni, Tejas, Ankush Gupta, Catalin Ionescu, Sebastian Borgeaud, Malcolm Reynolds, Andrew Zisserman, and Volodymyr Mnih. 2019. “Unsupervised Learning of Object Keypoints for Perception and Control.” arXiv. http://arxiv.org/abs/1906.11883.                                                                        |      |
|       | Lai, Siqi, Zhao Xu, Weijia Zhang, Hao Liu, and Hui Xiong. 2024. “LLMLight: Large Language Models as Traffic Signal Control Agents.” arXiv. http://arxiv.org/abs/2312.16044.                                                                                                                                              |      |
|       | Laidlaw, Cassidy, Banghua Zhu, Stuart Russell, and Anca Dragan. 2023. “The Effective Horizon Explains Deep RL Performance in Stochastic Environments.” arXiv. http://arxiv.org/abs/2312.08369.                                                                                                                           |      |
|       | Lee, Alex X., Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. 2020. “Stochastic Latent Actor-Critic: Deep Reinforcement Learning with a Latent Variable Model.” arXiv. http://arxiv.org/abs/1907.00953.                                                                                                              |      |
|       | Léger, Corentin, Gautier Hamon, Eleni Nisioti, Xavier Hinaut, and Clément Moulin-Frier. 2024. “Evolving Reservoirs for Meta Reinforcement Learning.” arXiv. http://arxiv.org/abs/2312.06695.                                                                                                                             |      |
|       | Leibo, Joel Z., Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. 2017. “Multi-Agent Reinforcement Learning in Sequential Social Dilemmas.” arXiv. http://arxiv.org/abs/1702.03037.                                                                                                                    |      |
|       | Li, Hao, Xiao-Hu Zhou, Xiao-Liang Xie, Shi-Qi Liu, Zhen-Qiu Feng, Xiao-Yin Liu, Mei-Jiang Gui, et al. 2023. “CROP: Conservative Reward for Model-Based Offline Policy Optimization.” arXiv. http://arxiv.org/abs/2310.17245.                                                                                             |      |
|       | Li, Jiwei, Will Monroe, Tianlin Shi, Sébastien Jean, Alan Ritter, and Dan Jurafsky. 2017. “Adversarial Learning for Neural Dialogue Generation.” arXiv. http://arxiv.org/abs/1701.06547.                                                                                                                                 |      |
|       | Li, Qiyang, Jason Zhang, Dibya Ghosh, Amy Zhang, and Sergey Levine. 2023. “Accelerating Exploration with Unlabeled Prior Data.” arXiv. http://arxiv.org/abs/2311.05067.                                                                                                                                                  |      |
|       | Li, Yu-Jhe, Hsin-Yu Chang, Yu-Jing Lin, Po-Wei Wu, and Yu-Chiang Frank Wang. 2018. “Deep Reinforcement Learning for Playing 2.5D Fighting Games.” arXiv. http://arxiv.org/abs/1805.02070.                                                                                                                                |      |
|       | Li, Yuxi. 2018. “Deep Reinforcement Learning.” arXiv. http://arxiv.org/abs/1810.06339.                                                                                                                                                                                                                                   |      |
|       | Lillicrap, Timothy P., Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2019. “Continuous Control with Deep Reinforcement Learning.” arXiv. http://arxiv.org/abs/1509.02971.                                                                                  |      |
|       | Liu, Evan Zheran, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. 2018. “Reinforcement Learning on Web Interfaces Using Workflow-Guided Exploration.” arXiv. http://arxiv.org/abs/1802.08802.                                                                                                                |      |
|       | Liu, Feng, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun Chen, Huifeng Guo, and Yuzhou Zhang. 2019. “Deep Reinforcement Learning Based Recommendation with Explicit User-Item Interactions Modeling.” arXiv. http://arxiv.org/abs/1810.12027.                                                                 |      |
|       | Liu, Xiangyu, Chenghao Deng, Yanchao Sun, Yongyuan Liang, and Furong Huang. 2024. “Beyond Worst-Case Attacks: Robust RL with Adaptive Defense via Non-Dominated Policies.” arXiv. http://arxiv.org/abs/2402.12673.                                                                                                       |      |
|       | Liu, Xiaodong, Yelong Shen, Kevin Duh, and Jianfeng Gao. 2018. “Stochastic Answer Networks for Machine Reading Comprehension.” arXiv. http://arxiv.org/abs/1712.03556.                                                                                                                                                   |      |
|       | Liu, Xiao-Yang, Hongyang Yang, Qian Chen, Runjia Zhang, Liuqing Yang, Bowen Xiao, and Christina Dan Wang. 2022. “FinRL: A Deep Reinforcement Learning Library for Automated Stock Trading in Quantitative Finance.” arXiv. http://arxiv.org/abs/2011.09607.                                                              |      |
|       | Liu, Zhishuai, and Pan Xu. 2024. “Distributionally Robust Off-Dynamics Reinforcement Learning: Provable Efficiency with Linear Function Approximation.” arXiv. http://arxiv.org/abs/2402.15399.                                                                                                                          |      |
|       | Lu, Chengjie, Tao Yue, Man Zhang, and Shaukat Ali. 2023. “DeepQTest: Testing Autonomous Driving Systems with Reinforcement Learning and Real-World Weather Data.” arXiv. http://arxiv.org/abs/2310.05170.                                                                                                                |      |
|       | Malekzadeh, Parvin, Konstantinos N. Plataniotis, Zissis Poulos, and Zeyu Wang. 2024. “A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning.” arXiv. http://arxiv.org/abs/2401.02325.                                                                            |      |
|       | Mallick, Samuel, Filippo Airaldi, Azita Dabiri, and Bart De Schutter. 2024. “Multi-Agent Reinforcement Learning via Distributed MPC as a Function Approximator.” arXiv. http://arxiv.org/abs/2312.05166.                                                                                                                 |      |
|       | Manczak, Blazej, Jan Viebahn, and Herke van Hoof. 2023. “Hierarchical Reinforcement Learning for Power Network Topology Control.” arXiv. http://arxiv.org/abs/2311.02129.                                                                                                                                                |      |
|       | Mao, Hangyu, Rui Zhao, Ziyue Li, Zhiwei Xu, Hao Chen, Yiqun Chen, Bin Zhang, Zhen Xiao, Junge Zhang, and Jiangjin Yin. 2023. “PDiT: Interleaving Perception and Decision-Making Transformers for Deep Reinforcement Learning.” arXiv. http://arxiv.org/abs/2312.15863.                                                   |      |
|       | Mark, Max Sobol, Archit Sharma, Fahim Tajwar, Rafael Rafailov, Sergey Levine, and Chelsea Finn. 2023. “Offline Retraining for Online RL: Decoupled Policy Learning to Mitigate Exploration Bias.” arXiv. http://arxiv.org/abs/2310.08558.                                                                                |      |
|       | Markowitz, Jared, and Edward W. Staley. 2023. “Clipped-Objective Policy Gradients for Pessimistic Policy Optimization.” arXiv. http://arxiv.org/abs/2311.05846.                                                                                                                                                          |      |
|       | Mathes, Timothy K., Jessica Inman, Andrés Colón, and Simon Khan. 2023. “CODEX: A Cluster-Based Method for Explainable Reinforcement Learning.” arXiv. http://arxiv.org/abs/2312.04216.                                                                                                                                   |      |
|       | Matsunaga, Daiki E., Jongmin Lee, Jaeseok Yoon, Stefanos Leonardos, Pieter Abbeel, and Kee-Eung Kim. 2023. “AlberDICE: Addressing Out-Of-Distribution Joint Actions in Offline Multi-Agent RL via Alternating Stationary Distribution Correction Estimation.” arXiv. http://arxiv.org/abs/2311.02194.                    |      |
|       | McCallum, Sabrina, Max Taylor-Davies, Stefano V. Albrecht, and Alessandro Suglia. 2023. “Is Feedback All You Need? Leveraging Natural Language Feedback in Goal-Conditioned Reinforcement Learning.” arXiv. http://arxiv.org/abs/2312.04736.                                                                             |      |
|       | Medina-Ramírez, Miguel Ángel, Cayetano Guerra-Artal, and Mario Hernández-Tejera. 2023. “Improving Dialogue Management: Quality Datasets vs Models.” arXiv. http://arxiv.org/abs/2310.01339.                                                                                                                              |      |
|       | Mediratta, Ishita, Minqi Jiang, Jack Parker-Holder, Michael Dennis, Eugene Vinitsky, and Tim Rocktäschel. 2023. “Stabilizing Unsupervised Environment Design with a Learned Adversary.” arXiv. http://arxiv.org/abs/2308.10797.                                                                                          |      |
|       | Mediratta, Ishita, Qingfei You, Minqi Jiang, and Roberta Raileanu. 2023. “The Generalization Gap in Offline Reinforcement Learning.” arXiv. http://arxiv.org/abs/2312.05742.                                                                                                                                             |      |
|       | Mirowski, Piotr, Matthew Koichi Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, Koray Kavukcuoglu, Andrew Zisserman, and Raia Hadsell. 2019. “Learning to Navigate in Cities Without a Map.” arXiv. http://arxiv.org/abs/1804.00168.                                  |      |
|       | Mnih, Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, et al. 2015. “Human-Level Control through Deep Reinforcement Learning.” Nature 518 (7540): 529–33. https://doi.org/10.1038/nature14236.                                                                   |      |
|       | Moritz, Philipp, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, et al. 2018. “Ray: A Distributed Framework for Emerging AI Applications.” arXiv. http://arxiv.org/abs/1712.05889.                                                                                             |      |
|       | Nachum, Ofir, Shixiang Gu, Honglak Lee, and Sergey Levine. 2019. “Near-Optimal Representation Learning for Hierarchical Reinforcement Learning.” arXiv. http://arxiv.org/abs/1810.01257.                                                                                                                                 |      |
|       | Nagabandi, Anusha, Gregory Kahn, Ronald S. Fearing, and Sergey Levine. 2018. “Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning.” arXiv:1708.02596 [Cs]. http://arxiv.org/abs/1708.02596.                                                                                  |      |
|       | Nair, Ashvin, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. 2021. “AWAC: Accelerating Online Reinforcement Learning with Offline Datasets.” arXiv. https://doi.org/10.48550/arXiv.2006.09359.                                                                                                                        |      |
|       | Najarro, Elias, and Sebastian Risi. 2022. “Meta-Learning through Hebbian Plasticity in Random Networks.” arXiv. http://arxiv.org/abs/2007.02686.                                                                                                                                                                         |      |
|       | Nazari, Mohammadreza, Afshin Oroojlooy, Lawrence V. Snyder, and Martin Takáč. 2018. “Reinforcement Learning for Solving the Vehicle Routing Problem.” arXiv. http://arxiv.org/abs/1802.04240.                                                                                                                            |      |
|       | Nehme, Ghadi, and Tejas Y. Deo. 2023. “Safe Navigation: Training Autonomous Vehicles Using Deep Reinforcement Learning in CARLA.” arXiv. http://arxiv.org/abs/2311.10735.                                                                                                                                                |      |
|       | Noukhovitch, Michael, Samuel Lavoie, Florian Strub, and Aaron Courville. 2023. “Language Model Alignment with Elastic Reset.” arXiv. http://arxiv.org/abs/2312.07551.                                                                                                                                                    |      |
|       | Nousiainen, Jalo, Byron Engler, Markus Kasper, Chang Rajani, Tapio Helin, Cédric T. Heritier, Sascha P. Quanz, and Adrian M. Glauser. 2023. “Laboratory Experiments of Model-Based Reinforcement Learning for Adaptive Optics Control.” arXiv. http://arxiv.org/abs/2401.00242.                                          |      |
|       | Osband, Ian, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. 2016. “Deep Exploration via Bootstrapped DQN.” arXiv. http://arxiv.org/abs/1602.04621.                                                                                                                                                           |      |
|       | Pan, Xinlei, Yurong You, Ziyan Wang, and Cewu Lu. 2017. “Virtual to Real Reinforcement Learning for Autonomous Driving.” arXiv. http://arxiv.org/abs/1704.03952.                                                                                                                                                         |      |
|       | Panaganti, Kishan, Zaiyan Xu, Dileep Kalathil, and Mohammad Ghavamzadeh. 2023. “Bridging Distributionally Robust Learning and Offline RL: An Approach to Mitigate Distribution Shift and Partial Data Coverage.” arXiv. http://arxiv.org/abs/2310.18434.                                                                 |      |
|       | Papangelis, Alexandros, Yi-Chia Wang, Piero Molino, and Gokhan Tur. 2019. “Collaborative Multi-Agent Dialogue Model Training Via Reinforcement Learning.” arXiv. http://arxiv.org/abs/1907.05507.                                                                                                                        |      |
|       | Papoudakis, Georgios, Filippos Christianos, Lukas Schäfer, and Stefano V. Albrecht. 2021. “Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks.” arXiv. http://arxiv.org/abs/2006.07869.                                                                                                |      |
|       | Parisotto, Emilio, H. Francis Song, Jack W. Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant M. Jayakumar, Max Jaderberg, et al. 2019. “Stabilizing Transformers for Reinforcement Learning.” arXiv. http://arxiv.org/abs/1910.06764.                                                                                      |      |
|       | Park, Seohong, Tobias Kreiman, and Sergey Levine. 2024. “Foundation Policies with Hilbert Representations.” arXiv. http://arxiv.org/abs/2402.15567.                                                                                                                                                                      |      |
|       | Park, Seohong, Oleh Rybkin, and Sergey Levine. 2023. “METRA: Scalable Unsupervised RL with Metric-Aware Abstraction.” arXiv. http://arxiv.org/abs/2310.08887.                                                                                                                                                            |      |
|       | Paul, Swarna Kamal. 2023. “Sequential Planning in Large Partially Observable Environments Guided by LLMs.”                                                                                                                                                                                                               |      |
|       | Peng, Xue Bin, Pieter Abbeel, Sergey Levine, and Michiel van de Panne. 2018. “DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills.” ACM Transactions on Graphics 37 (4): 1–14. https://doi.org/10.1145/3197517.3201311.                                                              |      |
|       | Peng, Xue Bin, Aviral Kumar, Grace Zhang, and Sergey Levine. 2019. “Advantage-Weighted Regression: Simple and Scalable off-Policy Reinforcement Learning.” arXiv Preprint arXiv:1910.00177.                                                                                                                              |      |
|       | Perolat, Julien, Joel Z. Leibo, Vinicius Zambaldi, Charles Beattie, Karl Tuyls, and Thore Graepel. 2017. “A Multi-Agent Reinforcement Learning Model of Common-Pool Resource Appropriation.” arXiv. http://arxiv.org/abs/1707.06600.                                                                                     |      |
|       | Peter, Silvan David. 2023. “Online Symbolic Music Alignment with Offline Reinforcement Learning,” November. https://doi.org/10.5281/zenodo.10265367.                                                                                                                                                                     |      |
|       | Petrenko, Aleksei, Zhehui Huang, Tushar Kumar, Gaurav Sukhatme, and Vladlen Koltun. 2020. “Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning.” arXiv. http://arxiv.org/abs/2006.11751.                                                                            |      |
|       | Pinto, Lerrel, James Davidson, Rahul Sukthankar, and Abhinav Gupta. 2017. “Robust Adversarial Reinforcement Learning.” arXiv. http://arxiv.org/abs/1703.02702.                                                                                                                                                           |      |
|       | Pritzel, Alexander, Benigno Uria, Sriram Srinivasan, Adrià Puigdomènech, Oriol Vinyals, Demis Hassabis, Daan Wierstra, and Charles Blundell. 2017. “Neural Episodic Control.” arXiv:1703.01988 [Cs, Stat], March. http://arxiv.org/abs/1703.01988.                                                                       |      |
|       | Raffin, Antonin, Ashley Hill, René Traoré, Timothée Lesort, Natalia Díaz-Rodríguez, and David Filliat. 2018. “S-RL Toolbox: Environments, Datasets and Evaluation Metrics for State Representation Learning.” arXiv. http://arxiv.org/abs/1809.09369.                                                                    |      |
|       | ———. 2019. “Decoupling Feature Extraction from Policy Learning: Assessing Benefits of State Representation Learning in Goal Based Robotics.” arXiv. http://arxiv.org/abs/1901.08651.                                                                                                                                     |      |
|       | Raffin, Antonin, Jens Kober, and Freek Stulp. 2021. “Smooth Exploration for Robotic Reinforcement Learning.” arXiv. http://arxiv.org/abs/2005.05719.                                                                                                                                                                     |      |
|       | Rakelly, Kate, Aurick Zhou, Deirdre Quillen, Chelsea Finn, and Sergey Levine. 2019. “Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables.” arXiv. http://arxiv.org/abs/1903.08254.                                                                                                      |      |
|       | Rashid, Tabish, Gregory Farquhar, Bei Peng, and Shimon Whiteson. 2020. “Weighted QMIX: Expanding Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning.” arXiv. http://arxiv.org/abs/2006.10800.                                                                                            |      |
|       | Reddy, Siddharth, Anca D. Dragan, and Sergey Levine. 2019. “SQIL: Imitation Learning via Reinforcement Learning with Sparse Rewards.” arXiv. http://arxiv.org/abs/1905.11108.                                                                                                                                            |      |
|       | Rigter, Marc, Jun Yamada, and Ingmar Posner. 2023. “World Models via Policy-Guided Trajectory Diffusion.” arXiv. http://arxiv.org/abs/2312.08533.                                                                                                                                                                        |      |
|       | Riquelme, Carlos, George Tucker, and Jasper Snoek. 2018. “Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling.” arXiv. http://arxiv.org/abs/1802.09127.                                                                                                              |      |
|       | Risi, Sebastian, and Kenneth O. Stanley. 2019. “Deep Neuroevolution of Recurrent and Discrete World Models.” arXiv. http://arxiv.org/abs/1906.08857.                                                                                                                                                                     |      |
|       | Rosen, Daniel, Illa Rochez, Caleb McIrvin, Joshua Lee, Kevin D’Alessandro, Max Wiecek, Nhan Hoang, et al. 2023. “RFRL Gym: A Reinforcement Learning Testbed for Cognitive Radio Applications.” https://doi.org/10.1109/ICMLA58977.2023.00046.                                                                            |      |
|       | Rutherford, Alexander, Benjamin Ellis, Matteo Gallici, Jonathan Cook, Andrei Lupu, Gardar Ingvarsson, Timon Willi, et al. 2023. “JaxMARL: Multi-Agent RL Environments in JAX.” arXiv. http://arxiv.org/abs/2311.10090.                                                                                                   |      |
|       | Sanchez, Francisco Roldan, Qiang Wang, David Cordova Bulens, Kevin McGuinness, Stephen Redmond, and Noel O’Connor. 2023. “Learning and Reusing Primitive Behaviours to Improve Hindsight Experience Replay Sample Efficiency.” arXiv. http://arxiv.org/abs/2310.01827.                                                   |      |
|       | Schaarschmidt, Michael, Alexander Kuhnle, Ben Ellis, Kai Fricke, Felix Gessert, and Eiko Yoneki. 2018. “LIFT: Reinforcement Learning in Computer Systems by Learning From Demonstrations.” arXiv. http://arxiv.org/abs/1808.07903.                                                                                       |      |
|       | Schiller, Christian A. 2023. “Virtual Augmented Reality for Atari Reinforcement Learning.” arXiv. http://arxiv.org/abs/2310.08683.                                                                                                                                                                                       |      |
|       | Schumacher, Pierre, Thomas Geijtenbeek, Vittorio Caggiano, Vikash Kumar, Syn Schmitt, Georg Martius, and Daniel F. B. Haeufle. 2023. “Natural and Robust Walking Using Reinforcement Learning without Demonstrations in High-Dimensional Musculoskeletal Models.” arXiv. http://arxiv.org/abs/2309.02976.                |      |
|       | Sekar, Ramanan, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, and Deepak Pathak. 2020. “Planning to Explore via Self-Supervised World Models.” arXiv. http://arxiv.org/abs/2005.05960.                                                                                                                  |      |
|       | Sermanet, Pierre, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, and Sergey Levine. 2018. “Time-Contrastive Networks: Self-Supervised Learning from Video.” arXiv. http://arxiv.org/abs/1704.06888.                                                                                                |      |
|       | Shea, Ryan, and Zhou Yu. 2023. “Building Persona Consistent Dialogue Agents with Offline Reinforcement Learning.” arXiv. http://arxiv.org/abs/2310.10735.                                                                                                                                                                |      |
|       | Shi, Ruizhe, Yuyao Liu, Yanjie Ze, Simon S. Du, and Huazhe Xu. 2023. “Unleashing the Power of Pre-Trained Language Models for Offline Reinforcement Learning.” arXiv. http://arxiv.org/abs/2310.20587.                                                                                                                   |      |
|       | Sigal, Adam, Hsiu-Chin Lin, and AJung Moon. 2024. “Improving Generalization in Reinforcement Learning Training Regimes for Social Robot Navigation.” arXiv. http://arxiv.org/abs/2308.14947.                                                                                                                             |      |
|       | Sinha, Samarth, Homanga Bharadhwaj, Aravind Srinivas, and Animesh Garg. 2020. “D2RL: Deep Dense Architectures in Reinforcement Learning.” arXiv. http://arxiv.org/abs/2010.09163.                                                                                                                                        |      |
|       | Srinivas, Aravind, Michael Laskin, and Pieter Abbeel. 2020. “CURL: Contrastive Unsupervised Representations for Reinforcement Learning.” arXiv. http://arxiv.org/abs/2004.04136.                                                                                                                                         |      |
|       | Srivastava, Rupesh Kumar, Pranav Shyam, Filipe Mutz, Wojciech Jaśkowski, and Jürgen Schmidhuber. 2021. “Training Agents Using Upside-Down Reinforcement Learning.” arXiv. http://arxiv.org/abs/1912.02877.                                                                                                               |      |
|       | Stadie, Bradly C., Ge Yang, Rein Houthooft, Xi Chen, Yan Duan, Yuhuai Wu, Pieter Abbeel, and Ilya Sutskever. 2019. “Some Considerations on Learning to Explore via Meta-Reinforcement Learning.” arXiv. http://arxiv.org/abs/1803.01118.                                                                                 |      |
|       | Stooke, Adam, and Pieter Abbeel. 2019. “Accelerated Methods for Deep Reinforcement Learning,” January. http://arxiv.org/abs/1803.02811.                                                                                                                                                                                  |      |
|       | Sunehag, Peter, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, et al. 2017. “Value-Decomposition Networks For Cooperative Multi-Agent Learning.” arXiv. http://arxiv.org/abs/1706.05296.                                                                        |      |
|       | Szot, Andrew, Alex Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, et al. 2022. “Habitat 2.0: Training Home Assistants to Rearrange Their Habitat.” arXiv. http://arxiv.org/abs/2106.14405.                                                                                                 |      |
|       | Tampuu, Ardi, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan Korjus, Juhan Aru, Jaan Aru, and Raul Vicente. 2015. “Multiagent Cooperation and Competition with Deep Reinforcement Learning.” arXiv. http://arxiv.org/abs/1511.08779.                                                                           |      |
|       | Tang, Huijie, Federico Berto, Zihan Ma, Chuanbo Hua, Kyuree Ahn, and Jinkyoo Park. 2024. “HiMAP: Learning Heuristics-Informed Policies for Large-Scale Multi-Agent Pathfinding.” arXiv. http://arxiv.org/abs/2402.15546.                                                                                                 |      |
|       | Tang, Shengpu, and Jenna Wiens. 2023. “Counterfactual-Augmented Importance Sampling for Semi-Offline Policy Evaluation.” arXiv. http://arxiv.org/abs/2310.17146.                                                                                                                                                         |      |
|       | Tassa, Yuval, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, et al. 2018. “DeepMind Control Suite.” arXiv. http://arxiv.org/abs/1801.00690.                                                                                                                                         |      |
|       | Tavakoli, Arash, Fabio Pardo, and Petar Kormushev. 2019. “Action Branching Architectures for Deep Reinforcement Learning.” arXiv. http://arxiv.org/abs/1711.08946.                                                                                                                                                       |      |
|       | Vecerik, Mel, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Rothörl, Thomas Lampe, and Martin Riedmiller. 2018. “Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards.” arXiv. http://arxiv.org/abs/1707.08817.            |      |
|       | Venkatraman, Siddarth, Shivesh Khaitan, Ravi Tej Akella, John Dolan, Jeff Schneider, and Glen Berseth. 2023. “Reasoning with Latent Diffusion in Offline Reinforcement Learning.” arXiv. http://arxiv.org/abs/2309.06599.                                                                                                |      |
|       | Vieillard, Nino, Olivier Pietquin, and Matthieu Geist. 2020. “Munchausen Reinforcement Learning.” arXiv. http://arxiv.org/abs/2007.14430.                                                                                                                                                                                |      |
|       | Vincent, Théo, Alberto Maria Metelli, Boris Belousov, Jan Peters, Marcello Restelli, and Carlo D’Eramo. 2024. “Parameterized Projected Bellman Operator.” arXiv. http://arxiv.org/abs/2312.12869.                                                                                                                        |      |
|       | Wang, Dong, and Giovanni Beltrame. 2024. “Deployable Reinforcement Learning with Variable Control Rate.” arXiv. http://arxiv.org/abs/2401.09286.                                                                                                                                                                         |      |
|       | Wang, Hanjing, Man-Kit Sit, Congjie He, Ying Wen, Weinan Zhang, Jun Wang, Yaodong Yang, and Luo Mai. 2023. “GEAR: A GPU-Centric Experience Replay System for Large Reinforcement Learning Models.” arXiv. http://arxiv.org/abs/2310.05205.                                                                               |      |
|       | Wang, Haoran, Yaoru Sun, Fang Wang, and Yeming Chen. 2023. “Guided Cooperation in Hierarchical Reinforcement Learning via Model-Based Rollout.” arXiv. http://arxiv.org/abs/2309.13508.                                                                                                                                  |      |
|       | Wang, Jane X., Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z. Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, and Matt Botvinick. 2016. “Learning to Reinforcement Learn.” arXiv:1611.05763 [Cs, Stat]. http://arxiv.org/abs/1611.05763.                                                                |      |
|       | Wang, Jiashuo, Haozhao Wang, Shichao Sun, and Wenjie Li. 2024. “Aligning Language Models with Human Preferences via a Bayesian Approach.” arXiv. http://arxiv.org/abs/2310.05782.                                                                                                                                        |      |
|       | Wang, Maonan, Xi Xiong, Yuheng Kan, Chengcheng Xu, and Man-On Pun. 2023. “UniTSA: A Universal Reinforcement Learning Framework for V2X Traffic Signal Control.” arXiv. http://arxiv.org/abs/2312.05090.                                                                                                                  |      |
|       | Wang, Shenzhi, Qisen Yang, Jiawei Gao, Matthieu Gaetan Lin, Hao Chen, Liwei Wu, Ning Jia, Shiji Song, and Gao Huang. 2023. “Train Once, Get a Family: State-Adaptive Balances for Offline-to-Online Reinforcement Learning.” arXiv. http://arxiv.org/abs/2310.17966.                                                     |      |
|       | Wang, Zihang, and Maowei Jiang. 2023. “Enhancing Data Efficiency in Reinforcement Learning: A Novel Imagination Mechanism Based on Mesh Information Propagation.” arXiv. http://arxiv.org/abs/2309.14243.                                                                                                                |      |
|       | Wang, Ziyu, Alexander Novikov, Konrad Zolna, Jost Tobias Springenberg, Scott Reed, Bobak Shahriari, Noah Siegel, et al. 2021. “Critic Regularized Regression.” arXiv. https://doi.org/10.48550/arXiv.2006.15134.                                                                                                         |      |
|       | Wang, Ziyu, Yanjie Ze, Yifei Sun, Zhecheng Yuan, and Huazhe Xu. 2023. “Generalizable Visual Reinforcement Learning with Segment Anything Model.” arXiv. http://arxiv.org/abs/2312.17116.                                                                                                                                 |      |
|       | Weerakoon, Kasun, Adarsh Jagan Sathyamoorthy, Mohamed Elnoor, and Dinesh Manocha. 2023. “VAPOR: Legged Robot Navigation in Outdoor Vegetation Using Offline Reinforcement Learning.” arXiv. http://arxiv.org/abs/2309.07832.                                                                                             |      |
|       | Wheeldon, Adrian, Alex Yakovlev, and Rishad Shafik. 2021. “Self-Timed Reinforcement Learning Using Tsetlin Machine.” arXiv. http://arxiv.org/abs/2109.00846.                                                                                                                                                             |      |
|       | Witt, Christian Schroeder de, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip H. S. Torr, Mingfei Sun, and Shimon Whiteson. 2020. “Is Independent Learning All You Need in the StarCraft Multi-Agent Challenge?” arXiv. http://arxiv.org/abs/2011.09533.                                                      |      |
|       | Wu, Yuhuai, Elman Mansimov, Shun Liao, Roger Grosse, and Jimmy Ba. 2017. “Scalable Trust-Region Method for Deep Reinforcement Learning Using Kronecker-Factored Approximation.” arXiv. http://arxiv.org/abs/1708.05144.                                                                                                  |      |
|       | Wydmuch, Marek, Michał Kempka, and Wojciech Jaśkowski. 2019. “ViZDoom Competitions: Playing Doom from Pixels.” IEEE Transactions on Games 11 (3): 248–59. https://doi.org/10.1109/TG.2018.2877047.                                                                                                                       |      |
|       | Xiao, Wenli, Tairan He, John Dolan, and Guanya Shi. 2024. “Safe Deep Policy Adaptation.” arXiv. http://arxiv.org/abs/2310.08602.                                                                                                                                                                                         |      |
|       | Xie, Tianbao, Siheng Zhao, Chen Henry Wu, Yitao Liu, Qian Luo, Victor Zhong, Yanchao Yang, and Tao Yu. 2023. “Text2Reward: Automated Dense Reward Function Generation for Reinforcement Learning.” arXiv. http://arxiv.org/abs/2309.11489.                                                                               |      |
|       | Xiong, Jiechao, Qing Wang, Zhuoran Yang, Peng Sun, Lei Han, Yang Zheng, Haobo Fu, Tong Zhang, Ji Liu, and Han Liu. 2018. “Parametrized Deep Q-Networks Learning: Reinforcement Learning with Discrete-Continuous Hybrid Action Space.” arXiv. http://arxiv.org/abs/1810.06394.                                           |      |
|       | Xu, Guowei, Ruijie Zheng, Yongyuan Liang, Xiyao Wang, Zhecheng Yuan, Tianying Ji, Yu Luo, et al. 2024. “DrM: Mastering Visual Reinforcement Learning through Dormant Ratio Minimization.” arXiv. http://arxiv.org/abs/2310.19668.                                                                                        |      |
|       | Yang, Derek, Li Zhao, Zichuan Lin, Tao Qin, Jiang Bian, and Tieyan Liu. 2020. “Fully Parameterized Quantile Function for Distributional Reinforcement Learning.” arXiv. http://arxiv.org/abs/1911.02140.                                                                                                                 |      |
|       | Yang, Junming, Xingguo Chen, Shengyuan Wang, and Bolei Zhang. 2023. “Model-Based Offline Policy Optimization with Adversarial Network.” arXiv. http://arxiv.org/abs/2309.02157.                                                                                                                                          |      |
|       | Yang, Rui, Han Zhong, Jiawei Xu, Amy Zhang, Chongjie Zhang, Lei Han, and Tong Zhang. 2024. “Towards Robust Offline Reinforcement Learning under Diverse Data Corruption.” arXiv. http://arxiv.org/abs/2310.12955.                                                                                                        |      |
|       | Yarats, Denis, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto. 2021. “Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning.” arXiv. https://doi.org/10.48550/arXiv.2107.09645.                                                                                                          |      |
|       | Ye, Chenlu, Rui Yang, Quanquan Gu, and Tong Zhang. 2024. “Corruption-Robust Offline Reinforcement Learning with General Function Approximation.” arXiv. http://arxiv.org/abs/2310.14550.                                                                                                                                 |      |
|       | Yu, Tianhe, Deirdre Quillen, Zhanpeng He, Ryan Julian, Avnish Narayan, Hayden Shively, Adithya Bellathur, Karol Hausman, Chelsea Finn, and Sergey Levine. 2021. “Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning.” arXiv. http://arxiv.org/abs/1910.10897.                         |      |
|       | Yu, Tianhe, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. 2020. “MOPO: Model-Based Offline Policy Optimization.” arXiv. http://arxiv.org/abs/2005.13239.                                                                                                              |      |
|       | Yu, Yuanqing, Chongming Gao, Jiawei Chen, Heng Tang, Yuefeng Sun, Qian Chen, Weizhi Ma, and Min Zhang. 2024. “EasyRL4Rec: A User-Friendly Code Library for Reinforcement Learning Based Recommender Systems.” arXiv. http://arxiv.org/abs/2402.15164.                                                                    |      |
|       | Yuan, Mingqi, Zequn Zhang, Yang Xu, Shihao Luo, Bo Li, Xin Jin, and Wenjun Zeng. 2023. “RLLTE: Long-Term Evolution Project of Reinforcement Learning.” arXiv. http://arxiv.org/abs/2309.16382.                                                                                                                           |      |
|       | Yuan, Xingdi, Tong Wang, Caglar Gulcehre, Alessandro Sordoni, Philip Bachman, Sandeep Subramanian, Saizheng Zhang, and Adam Trischler. 2017. “Machine Comprehension by Text-to-Text Neural Question Generation.” arXiv. http://arxiv.org/abs/1705.02012.                                                                 |      |
|       | Zambaldi, Vinicius, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, et al. 2018. “Relational Deep Reinforcement Learning.” arXiv. http://arxiv.org/abs/1806.01830.                                                                                                                      |      |
|       | Zangirolami, Valentina, and Matteo Borrotti. 2024. “Dealing with Uncertainty: Balancing Exploration and Exploitation in Deep Recurrent Reinforcement Learning.” arXiv. http://arxiv.org/abs/2310.08331.                                                                                                                  |      |
|       | Zeng, Andy, Shuran Song, Stefan Welker, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. 2018. “Learning Synergies between Pushing and Grasping with Self-Supervised Deep Reinforcement Learning.” arXiv. http://arxiv.org/abs/1803.09956.                                                                          |      |
|       | Zha, Daochen, Kwei-Herng Lai, Yuanpu Cao, Songyi Huang, Ruzhe Wei, Junyu Guo, and Xia Hu. 2020. “RLCard: A Toolkit for Reinforcement Learning in Card Games.” arXiv. http://arxiv.org/abs/1910.04376.                                                                                                                    |      |
|       | Zhan, Simon Sinong, Yixuan Wang, Qingyuan Wu, Ruochen Jiao, Chao Huang, and Qi Zhu. 2023. “State-Wise Safe Reinforcement Learning With Pixel Observations.” arXiv. http://arxiv.org/abs/2311.02227.                                                                                                                      |      |
|       | Zhang, Cong, Wen Song, Zhiguang Cao, Jie Zhang, Puay Siew Tan, and Chi Xu. 2020. “Learning to Dispatch for Job Shop Scheduling via Deep Reinforcement Learning.” arXiv. http://arxiv.org/abs/2010.12367.                                                                                                                 |      |
|       | Zhang, Huan, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui Hsieh. 2021. “Robust Deep Reinforcement Learning against Adversarial Perturbations on State Observations.” arXiv. http://arxiv.org/abs/2003.08938.                                                                                 |      |
|       | Zhang, Kaiqing, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Başar. 2018. “Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents.” arXiv. http://arxiv.org/abs/1802.08757.                                                                                                                        |      |
|       | Zhang, Shangtong, and Richard S. Sutton. 2018. “A Deeper Look at Experience Replay.” arXiv. http://arxiv.org/abs/1712.01275.                                                                                                                                                                                             |      |
|       | Zhang, Shenao, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo Yuan, Yingxiang Yang, Hongxia Yang, and Zhaoran Wang. 2024. “How Can LLM Guide RL? A Value-Based Approach.” arXiv. http://arxiv.org/abs/2402.16181.                                                                                                 |      |
|       | Zhang, Wentao, Yilei Zhao, Shuo Sun, Jie Ying, Yonggang Xie, Zitao Song, Xinrun Wang, and Bo An. 2024. “Reinforcement Learning with Maskable Stock Representation for Portfolio Management in Customizable Stock Pools.” arXiv. http://arxiv.org/abs/2311.10801.                                                         |      |
|       | Zhao, Kesen, Shuchang Liu, Qingpeng Cai, Xiangyu Zhao, Ziru Liu, Dong Zheng, Peng Jiang, and Kun Gai. 2023. “KuaiSim: A Comprehensive Simulator for Recommender Systems.” arXiv. http://arxiv.org/abs/2309.12645.                                                                                                        |      |
|       | Zhao, Xiangyu, Liang Zhang, Long Xia, Zhuoye Ding, Dawei Yin, and Jiliang Tang. 2019. “Deep Reinforcement Learning for List-Wise Recommendations.” arXiv. http://arxiv.org/abs/1801.00209.                                                                                                                               |      |
|       | Zheng, Tianyu, Ge Zhang, Xingwei Qu, Ming Kuang, Stephen W. Huang, and Zhaofeng He. 2024. “MORE-3S:Multimodal-Based Offline Reinforcement Learning with Shared Semantic Spaces.” arXiv. http://arxiv.org/abs/2402.12845.                                                                                                 |      |
|       | Zhong, Cheng, Kangenbei Liao, Wei Chen, Qianlong Liu, Baolin Peng, Xuanjing Huang, Jiajie Peng, and Zhongyu Wei. 2023. “Hierarchical Reinforcement Learning for Automatic Disease Diagnosis.” arXiv. http://arxiv.org/abs/2004.14254.                                                                                    |      |
|       | Zhou, Bei, and Soren Riis. 2024. “Exploring Parity Challenges in Reinforcement Learning through Curriculum Learning with Noisy Labels.” arXiv. http://arxiv.org/abs/2312.05379.                                                                                                                                          |      |
|       | Zhou, Kaiyang, Yu Qiao, and Tao Xiang. 2018. “Deep Reinforcement Learning for Unsupervised Video Summarization with Diversity-Representativeness Reward.” arXiv. http://arxiv.org/abs/1801.00054.                                                                                                                        |      |
|       | Zhou, Zhenpeng, Steven Kearnes, Li Li, Richard N. Zare, and Patrick Riley. 2019. “Optimization of Molecules via Deep Reinforcement Learning.” Scientific Reports 9 (1): 10752. https://doi.org/10.1038/s41598-019-47148-x.                                                                                               |      |
|       | Zhou, Zihao, Bin Hu, Chenyang Zhao, Pu Zhang, and Bin Liu. 2024. “Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents.” arXiv. http://arxiv.org/abs/2311.13373.                                                                                                                          |      |
|       | Zhu, Zhengbang, Hanye Zhao, Haoran He, Yichao Zhong, Shenyu Zhang, Haoquan Guo, Tingting Chen, and Weinan Zhang. 2024. “Diffusion Models for Reinforcement Learning: A Survey.” arXiv. http://arxiv.org/abs/2311.01223.                                                                                                  |      |
|       | Zhu, Zheqing, Rodrigo de Salvo Braz, Jalaj Bhandari, Daniel Jiang, Yi Wan, Yonathan Efroni, Liyuan Wang, et al. 2023. “Pearl: A Production-Ready Reinforcement Learning Agent.” arXiv. http://arxiv.org/abs/2312.03814.                                                                                                  |      |
|       | Ziegler, Daniel M., Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2020. “Fine-Tuning Language Models from Human Preferences.” arXiv. http://arxiv.org/abs/1909.08593.                                                                                      |      |



1. **Deep Q-Network (DQN)**
2. **Double DQN**
3. **Dueling DQN**
4. **Prioritized Experience Replay DQN**
5. **Multi-step DQN**
6. **Rainbow DQN** (combines several DQN enhancements)
7. **Quantile Regression DQN**
8. **Implicit Quantile Networks (IQN)**
9. **Deep Deterministic Policy Gradient (DDPG)**
10. **Twin Delayed DDPG (TD3)**
11. **Distributed DDPG**
12. **Asynchronous Advantage Actor-Critic (A3C)**
13. **Synchronous Advantage Actor-Critic (A2C)**
14. **Soft Actor-Critic (SAC)**
15. **Hindsight Experience Replay (HER)**
16. **Trust Region Policy Optimization (TRPO)**
17. **Proximal Policy Optimization (PPO)**
18. **Generalized Advantage Estimation (GAE)**
19. **Actor-Critic with Experience Replay (ACER)**
20. **Sample Efficient Actor-Critic with Experience Replay (SEAC)**
21. **Recurrent Replay Distributed DQN (R2D2)**
22. **Monte Carlo Tree Search (MCTS) with DRL**
23. **Neural Episodic Control (NEC)**
24. **Model-based Policy Optimization (MBPO)**
25. **World Models**
26. **Dreamer Algorithm** (model-based RL)
27. **MuZero**
28. **Guided Policy Search (GPS)**
29. **Path Integral Guided Policy Search (PIGPS)**
30. **Curiosity-Driven Learning**
31. **Asynchronous Methods for Deep Learning (ADAM)**
32. **Natural Policy Gradient (NPG)**
33. **Distributed Proximal Policy Optimization (DPPO)**
34. **Importance Weighted Actor-Learner Architectures (IMPALA)**
35. **Self-Imitation Learning**
36. **Evolution Strategies as a Scalable Alternative to Reinforcement Learning**
37. **Soft Q-Learning**
38. **Option-Critic Architecture**
39. **Learning to Learn by Gradient Descent by Gradient Descent**
40. **UNREAL (Unsupervised Reinforcement and Auxiliary Learning)**
41. **The Predictron**
42. **Action Branching Architectures**
43. **Efficient Zero-shot Coordination (EZC)**
44. **Meta Learning Shared Hierarchies (MLSH)**
45. **Reward Constrained Policy Optimization (RCPO)**
46. **Episodic Curiosity through Reachability**
47. **Differentiable Algorithm Networks**
48. **Hierarchical Reinforcement Learning (HRL)**
49. **Successor Features for Transfer in Reinforcement Learning**
50. **Maximum a Posteriori Policy Optimization (MPO)**
