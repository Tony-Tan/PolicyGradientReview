- Early studies in RL
    - [ ] Witten, I. H. (1977). An adaptive optimal controller for discrete-time Markov environments. Information and Control, 34(4):286–295.
    - [ ] Barto, A. G., Sutton, R. S., Anderson, C. W. (1983). Neuronlike elements that can solve dicult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, 13(5):835–846. Reprinted in J. A. Anderson and E. Rosenfeld (Eds.), Neurocomputing: Foundations of Research, pp. 535–549. MIT Press, Cambridge, MA, 1988.
    - [ ] Sutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. Ph.D. thesis, University of Massachusetts, Amherst.
    - [ ] Williams, R. J. (1987). Reinforcement-learning connectionist systems. Technical Report NU-CCS-87-3. College of Computer Science, Northeastern University, Boston.
    - [ ] Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229–256.

- In predecessor field
    - [ ] Phansalkar, V. V., Thathachar, M. A. L. (1995). Local and global optimization algorithms for generalized learning automata. Neural Computation, 7(5):950–973.


- Natural-gradient methods
    - [ ] Amari, S. I. (1998). Natural gradient works eciently in learning. Neural Computation, 10 (2):251–276. 
    - [ ] Kakade, S. M. (2002). A natural policy gradient. In Advances in Neural Information Processing Systems 14 (NIPS 2001), pp. 1531–1538. MIT Press, Cambridge, MA.
    - [ ] Peters, J., Vijayakumar, S., Schaal, S. (2005). Natural actor–critic. In European Conference on Machine Learning, pp. 280–291. Springer Berlin Heidelberg
    - [ ] Peters, J., Schaal, S. (2008). Natural actor–critic. Neurocomputing, 71 (7):1180–1190. 
    - [ ] Park, J., Kim, J., Kang, D. (2005). An RLS-based natural actor–critic algorithm for locomotion of a two-linked robot arm. Computational Intelligence and Security:65–72.
    - [ ] Bhatnagar, S., Sutton, R., Ghavamzadeh, M., Lee, M. (2009). Natural actor–critic algorithms. Automatica, 45 (11).
    - [ ] Grondman, I., Busoniu, L., Lopes, G. A., Babuska, R. (2012). A survey of actor–critic reinforcement learning: Standard and natural policy gradients. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 42 (6):1291–1307

- deterministic policy gradient methods 
    - [ ] Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., Riedmiller, M. (2014). Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on Machine Learning (ICML 2014), pp. 387–395

- Off-policy policy-gradient methods
    - [ ] Degris, T., White, M., Sutton, R. S. (2012). Off-policy actor–critic. In Proceedings of the 29th International Conference on Machine Learning (ICML 2012). ArXiv:1205.4839, 2012.
    - [ ] Maei, H. R. (2018). Convergent actor-critic algorithms under o↵-policy training and function approximation. ArXiv:1802.07842.

- Entropy regularization
    - [ ] Schulman, J., Chen, X., Abbeel, P. (2017). Equivalence between policy gradients and soft Q-Learning. ArXiv:1704.06440.


- All above is based primarily on (who introduced the term "policy gradient methods")
    - [ ] Sutton, R. S., McAllester, D. A., Singh, S. P., Mansour, Y. (2000). Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12 (NIPS 1999), pp. 1057–1063. MIT Press, Cambridge, MA.



- Overview
    - [ ] Bhatnagar, S., Sutton, R., Ghavamzadeh, M., Lee, M. (2009). Natural actor–critic algorithms. Automatica, 45 (11).


- One of the earliest related works 
    - [ ] Aleksandrov, V. M., Sysoev, V. I., Shemeneva, V. V. (1968). Stochastic optimization of systems. Izv. Akad. Nauk SSSR, Tekh. Kibernetika:14–19.




- discounted in episodic 
    - [ ] Thomas, P. S. (2014). Bias in natural actor–critic algorithms. In Proceedings of the 31st International Conference on Machine Learning, JMLR W&CP 32 (1), pp. 441–448.




- Gradient theorem on page 334
    - [ ] Marbach, P., Tsitsiklis, J. N. (1998). Simulation-based optimization of Markov reward processes. MIT Technical Report LIDS-P-2411.
    - [ ] Marbach, P., Tsitsiklis, J. N. (2001). Simulation-based optimization of Markov reward processes. IEEE Transactions on Automatic Control, 46 (2):191–209
    - [ ] Sutton, R. S., Singh, S. P., McAllester, D. A. (2000). Comparing policy-gradient algorithms. Unpublished manuscript.
    - [ ] Cao, X. R., Chen, H. F. (1997). Perturbation realization, potentials, and sensitivity analysis of Markov processes. IEEE Transactions on Automatic Control, 42 (10):1382–1393.
    - [ ] Konda, V. R., Tsitsiklis, J. N. (2000). Actor-critic algorithms. In Advances in Neural Information Processing Systems 12 (NIPS 1999), pp. 1008–1014. MIT Press, Cambridge, MA. 
    - [ ] Konda, V. R., Tsitsiklis, J. N. (2003). On actor-critic algorithms. SIAM Journal on Control and Optimization, 42 (4):1143–1166.
    - [ ] Baxter, J., Bartlett, P. L. (2001). Infinite-horizon policy-gradient estimation. Journal of Artificial Intelligence Research, 15 :319–350.
    - [ ] Baxter, J., Bartlett, P. L., Weaver, L. (2001). Experiments with infinite-horizon, policy-gradient estimation. Journal of Artificial Intelligence Research, 15 :351–381.
    - [ ] Sutton, R. S., Singh, S. P., McAllester, D. A. (2000). Comparing policy-gradient algorithms. Unpublished manuscript.




- REINFORCEMENT 
    - [ ] Williams, R. J. (1987). Reinforcement-learning connectionist systems. Technical Report NU-CCS-87-3. College of Computer Science, Northeastern University, Boston.
    - [ ] Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229–256.
    - [ ] Phansalkar, V. V., Thathachar, M. A. L. (1995). Local and global optimization algorithms for generalized learning automata. Neural Computation, 7(5):950–973.

- All-actions algorithm
    - [ ] Sutton, R. S., Singh, S. P., McAllester, D. A. (2000). Comparing policy-gradient algorithms. Unpublished manuscript.


- Expected Policy Gradients
    - [ ] Ciosek, K., Whiteson, S. (2017). Expected policy gradients. ArXiv:1706.05374v1. A revised version appeared in Proceedings of the Annual Conference of the Association for the Advancement of Artificial Intelligence (AAAI-2018), pp. 2868–2875.
    - [ ] Ciosek, K., Whiteson, S. (2018). Expected policy gradients for reinforcement learning. ArXiv: 1801.03326.  

- Mean actor critic
    - [ ] Asadi, K., Allen, C., Roderick, M., Mohamed, A. R., Konidaris, G., Littman, M. (2017). Mean actor critic. ArXiv:1709.00503.



- Baseline
    - [ ] Williams, R. J. (1987). Reinforcement-learning connectionist systems. Technical Report NU-CCS-87-3. College of Computer Science, Northeastern University, Boston.
    - [ ] Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4):229–256.
    - [ ] Greensmith, E., Bartlett, P. L., Baxter, J. (2004). Variance reduction techniques for gradient estimates in reinforcement learning. Journal of Machine Learning Research, 5 (Nov):1471– 1530.
    - [ ] Dick, T. (2015). Policy Gradient Reinforcement Learning Without Regret. M.Sc. thesis, University of Alberta.
    - [ ] Thomas, P. S., Brunskill, E. (2017). Policy gradient methods for reinforcement learning with function approximation and action-dependent baselines. ArXiv:1706.06643.
    - [ ] 

- Actor-critic
    - [ ] Witten, I. H. (1977). An adaptive optimal controller for discrete-time Markov environments. Information and Control, 34(4):286–295.
    - [ ] Barto, A. G., Sutton, R. S., Anderson, C. W. (1983). Neuronlike elements that can solve dicult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, 13(5):835–846. Reprinted in J. A. Anderson and E. Rosenfeld (Eds.), Neurocomputing: Foundations of Research, pp. 535–549. MIT Press, Cambridge, MA, 1988.
    - [ ] Sutton, R. S. (1984). Temporal Credit Assignment in Reinforcement Learning. Ph.D. thesis, University of Massachusetts, Amherst.
    - [ ] Degris, T., White, M., Sutton, R. S. (2012). O↵-policy actor–critic. In Proceedings of the 29th International Conference on Machine Learning (ICML 2012). ArXiv:1205.4839, 2012.

- Continuous Actions
    - [ ] Williams, R. J. (1987). Reinforcement-learning connectionist systems. Technical Report NU-CCS-87-3. College of Computer Science, Northeastern University, Boston.
    - [ ] Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3-4)6666:229–256
